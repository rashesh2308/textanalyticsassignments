{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Data Set Description\n",
    "The questions below relate to the data files associated with the contest with the title 'DengAI: Predicting Disease Spread' published at the following website. \n",
    "https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/data/\n",
    "\n",
    "Anyone can join the contest and showcase your skills. To know about contest submissions visit the following webpage\n",
    "https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/submissions/\n",
    "You can showcase your Machine Learning skills by ranking top in the contest. \n",
    "\n",
    "Problem description:\n",
    "Your goal is to predict the total_cases label for each (city, year, weekofyear) in the test set. There are two cities, San Juan and Iquitos, with test data for each city spanning 5 and 3 years respectively. You will make one submission that contains predictions for both cities. The data for each city have been concatenated along with a city column indicating the source: sj for San Juan and iq for Iquitos. The test set is a pure future hold-out, meaning the test data are sequential and non-overlapping with any of the training data. Throughout, missing values have been filled as NaNs.\n",
    "\n",
    "Assignment:\n",
    "The goal is achieved through three subsequent Assignments 1, 2 and 3, all using the same dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset\n",
    "You are provided the following set of information on a (year, weekofyear) timescale:\n",
    "\n",
    "(Where appropriate, units are provided as a _unit suffix on the feature name.)\n",
    "\n",
    "City and date indicators\n",
    "\n",
    "    city – City abbreviations: sj for San Juan and iq for Iquitos\n",
    "    week_start_date – Date given in yyyy-mm-dd format\n",
    "\n",
    "NOAA's GHCN daily climate data weather station measurements\n",
    "\n",
    "    station_max_temp_c – Maximum temperature\n",
    "    station_min_temp_c – Minimum temperature\n",
    "    station_avg_temp_c – Average temperature\n",
    "    station_precip_mm – Total precipitation\n",
    "    station_diur_temp_rng_c – Diurnal temperature range\n",
    "    \n",
    "PERSIANN satellite precipitation measurements (0.25x0.25 degree scale)\n",
    "\n",
    "    precipitation_amt_mm – Total precipitation\n",
    "\n",
    "NOAA's NCEP Climate Forecast System Reanalysis measurements (0.5x0.5 degree scale)\n",
    "\n",
    "    reanalysis_sat_precip_amt_mm – Total precipitation\n",
    "    reanalysis_dew_point_temp_k – Mean dew point temperature\n",
    "    reanalysis_air_temp_k – Mean air temperature\n",
    "    reanalysis_relative_humidity_percent – Mean relative humidity\n",
    "    reanalysis_specific_humidity_g_per_kg – Mean specific humidity\n",
    "    reanalysis_precip_amt_kg_per_m2 – Total precipitation\n",
    "    reanalysis_max_air_temp_k – Maximum air temperature\n",
    "    reanalysis_min_air_temp_k – Minimum air temperature\n",
    "    reanalysis_avg_temp_k – Average air temperature\n",
    "    reanalysis_tdtr_k – Diurnal temperature range\n",
    "\n",
    "Satellite vegetation - Normalized difference vegetation index (NDVI) - NOAA's CDR Normalized Difference Vegetation Index (0.5x0.5 degree scale) measurements\n",
    "\n",
    "    ndvi_se – Pixel southeast of city centroid\n",
    "    ndvi_sw – Pixel southwest of city centroid\n",
    "    ndvi_ne – Pixel northeast of city centroid\n",
    "    ndvi_nw – Pixel northwest of city centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Questions\n",
    "Use the merged data frame from Assignment 1 (and 2) for this assignment\n",
    "\n",
    "This Assignment focuses on data preprocessing and model building. Continue with the datasets loaded in Assignment 1 and 2 (or reload with same steps and create merged data frame). In this assignment you need to use both i). Linear SVR regressor and ii) SVR with Linear kernel. Provide your intepretations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a brief statement: Is it required to consider the repeating features in the model? (For example: precipitation_amt_mm and reanalysis_sat_precip_amt_mm. Similarly there are others). List which features you will choose in this model. Select the required columns in the dataframe and drop the others.\n",
    "2. Preprocess the data \n",
    "    1. Encode the  categorical features\n",
    "    2. Abbreviate the column names\n",
    "    3. Standardize the numerical features\n",
    "    4. Handle NaNs (either fill them with Median values or make a better choice if available)\n",
    "    5. Convert percentage to value/100 (check which option gives better result, as-is or divided/100)\n",
    "    6. Scale the vegetation features (ndvi*) as 0, 1 (0 <= 0.25 and 1 > 0.25) as it is in scale of 0 to 0.5. Retain as is or use other scaling based on how model performs.\n",
    "    7. Make an 80-20 train-test split \n",
    "3. Build a Linear SVR regressor, train the model\n",
    "    1. Evaluate your model based on applicable metrics. Show the metric(s) you chose and why you chose this(these) metrics.\n",
    "    2. List the hyper-parameters that can be tuned in Linear SVR. Explain the meaning of each hyper-parameter. Show the code along with comments on the parameter value chosen (use class presentation, discussion notes, some online reading) and why this value was chosen. Show the improvement you achieved in model accuracy. \n",
    "    3. Plot Learning curve and provide insights\n",
    "4. Build a SVR model with Linear Kernel, train the model\n",
    "    1. Evaluate your model based on applicable metrics. Show the metric(s) you chose and why you chose this(these) metrics.\n",
    "    2. List the hyper-parameters that can be tuned in SVR model with Linear Kernel. Show the code along with comments on the parameter value chosen (use class presentation, discussion notes, some online reading) and why this value was chosen. Show the improvement you achieved in model accuracy. \n",
    "    3. List the hyper-parameters that can be tuned in SVR model with Linear Kernel. Show the code along with comments on the parameter value chosen (use class presentation, discussion notes, some online reading) and why this value was chosen. Show the improvement you achieved in model accuracy. \n",
    "5. Create a submission file which has predictions for both cities in the submission format prescribed by the contest at the link https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/data/. To get this, you need to use the file 'dengue_features_test.csv'\n",
    "6. Optional: Submit your predictions to the contest. You will get a submission score. Use any other models of your choice discusssed in the class and check if you get improved result. Show it here.\n",
    "7. Compare the Model Evaluation metric of SGD (Assignment 2), Linear SVR and SVR with Linear Kernel and state which model performed better along with reason why you consider so.\n",
    "\n",
    "Submit the following for this assignment: \n",
    "1. .ipynb and .html formats of Jupyter notebook code with outputs and \n",
    "2. the submission_format.csv with your predictions\n",
    "\n",
    "Note:\n",
    "Missing each of these will take out 0.1 point at each applicable place:\n",
    "1. Using full file path (you can use path=input() to set path and use it in os.chdir())\n",
    "2. Not labeling plots (x-label, y-label, title), not presentable plots (too small/oversized/overflow/unreadbale labels etc)\n",
    "3. Not having observation below each plot, dataframe output, model result/evaluation output where applies. Observation should be pointed, should not be generic statement and should be backed up with technical evidence from the results obtained.\n",
    "4. Not showing each question as markdown, then answer, then observation as markdown\n",
    "5. Printing full/lengthy dataframe\n",
    "6. Importing excess libraries and not used in code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
