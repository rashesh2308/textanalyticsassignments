{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Classification using MLP Classifier in sklearn\n",
    "\n",
    "We will predict the ocean proximity (`ocean_proximity` column) of Californian districts, given a number of features from these districts.\n",
    "\n",
    "**The unit of analysis is a DISTRICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.569704</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>2635.763081</td>\n",
       "      <td>537.870553</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>499.539680</td>\n",
       "      <td>3.870671</td>\n",
       "      <td>206855.816909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.003532</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2181.615252</td>\n",
       "      <td>421.385070</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>382.329753</td>\n",
       "      <td>1.899822</td>\n",
       "      <td>115395.615874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1447.750000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.563400</td>\n",
       "      <td>119600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>179700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>4.743250</td>\n",
       "      <td>264725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
       "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
       "std        2.003532      2.135952           12.585558   2181.615252   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
       "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
       "mean       537.870553   1425.476744    499.539680       3.870671   \n",
       "std        421.385070   1132.462122    382.329753       1.899822   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        296.000000    787.000000    280.000000       2.563400   \n",
       "50%        435.000000   1166.000000    409.000000       3.534800   \n",
       "75%        647.000000   1725.000000    605.000000       4.743250   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        20640.000000  \n",
       "mean        206855.816909  \n",
       "std         115395.615874  \n",
       "min          14999.000000  \n",
       "25%         119600.000000  \n",
       "50%         179700.000000  \n",
       "75%         264725.000000  \n",
       "max         500001.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.read_csv(\"housing.csv\")\n",
    "housing.head()\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20433 entries, 0 to 20432\n",
      "Data columns (total 10 columns):\n",
      "longitude             20433 non-null float64\n",
      "latitude              20433 non-null float64\n",
      "housing_median_age    20433 non-null float64\n",
      "total_rooms           20433 non-null float64\n",
      "total_bedrooms        20433 non-null float64\n",
      "population            20433 non-null float64\n",
      "households            20433 non-null float64\n",
      "median_income         20433 non-null float64\n",
      "median_house_value    20433 non-null float64\n",
      "ocean_proximity       20433 non-null object\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Drop the missing values\n",
    "housing.dropna(axis=0, inplace=True)\n",
    "\n",
    "# pandas specific issue - why you might want to reset the index\n",
    "# housing.reset_index(inplace=True, drop=True)\n",
    "\n",
    "housing.describe()\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     9034\n",
       "INLAND        6496\n",
       "NEAR OCEAN    2628\n",
       "NEAR BAY      2270\n",
       "ISLAND           5\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set the training and test data sets\n",
    "housing_X = housing.drop(\"ocean_proximity\", axis=1) # drop labels \n",
    "#Select the label\n",
    "housing_target = housing[\"ocean_proximity\"]\n",
    "housing_target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "housing_X_std = scaler.fit_transform(housing_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32731375,  1.05171726,  0.98216331, ..., -0.97683327,\n",
       "         2.34516291,  2.12881864],\n",
       "       [-1.32232256,  1.04235526, -0.60621017, ...,  1.67037262,\n",
       "         2.33263161,  1.31362603],\n",
       "       [-1.33230494,  1.03767426,  1.85576873, ..., -0.84342665,\n",
       "         1.78293943,  1.25818254],\n",
       "       ...,\n",
       "       [-0.82320322,  1.77727236, -0.92388486, ..., -0.17377773,\n",
       "        -1.14317103, -0.99247676],\n",
       "       [-0.87311515,  1.77727236, -0.84446619, ..., -0.39350628,\n",
       "        -1.05513604, -1.05831591],\n",
       "       [-0.83318561,  1.74918635, -1.00330353, ...,  0.07995643,\n",
       "        -0.78060586, -1.01759959]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_X_std.shape\n",
    "type(housing_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(housing_X_std, housing_target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14303, 9), (6130, 9))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bp/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Default settings create 1 hidden layer with 100 neurons\n",
    "#look at MLPClassifier documentation in sklearn to see which parameters can be modified\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,))\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase maximum iterations for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.23861184\n",
      "Iteration 2, loss = 0.87903884\n",
      "Iteration 3, loss = 0.76126467\n",
      "Iteration 4, loss = 0.68942403\n",
      "Iteration 5, loss = 0.63808049\n",
      "Iteration 6, loss = 0.59947966\n",
      "Iteration 7, loss = 0.56672548\n",
      "Iteration 8, loss = 0.53770058\n",
      "Iteration 9, loss = 0.51218707\n",
      "Iteration 10, loss = 0.49009063\n",
      "Iteration 11, loss = 0.47019942\n",
      "Iteration 12, loss = 0.45331776\n",
      "Iteration 13, loss = 0.43806849\n",
      "Iteration 14, loss = 0.42308956\n",
      "Iteration 15, loss = 0.41097041\n",
      "Iteration 16, loss = 0.39960711\n",
      "Iteration 17, loss = 0.39001973\n",
      "Iteration 18, loss = 0.38245169\n",
      "Iteration 19, loss = 0.37402374\n",
      "Iteration 20, loss = 0.36620794\n",
      "Iteration 21, loss = 0.36052631\n",
      "Iteration 22, loss = 0.35315666\n",
      "Iteration 23, loss = 0.34646159\n",
      "Iteration 24, loss = 0.34195153\n",
      "Iteration 25, loss = 0.33560149\n",
      "Iteration 26, loss = 0.33157849\n",
      "Iteration 27, loss = 0.32610294\n",
      "Iteration 28, loss = 0.32184937\n",
      "Iteration 29, loss = 0.31718464\n",
      "Iteration 30, loss = 0.31354321\n",
      "Iteration 31, loss = 0.31034364\n",
      "Iteration 32, loss = 0.30742901\n",
      "Iteration 33, loss = 0.30278535\n",
      "Iteration 34, loss = 0.30013373\n",
      "Iteration 35, loss = 0.29763532\n",
      "Iteration 36, loss = 0.29400946\n",
      "Iteration 37, loss = 0.29130505\n",
      "Iteration 38, loss = 0.28923326\n",
      "Iteration 39, loss = 0.28589288\n",
      "Iteration 40, loss = 0.28406061\n",
      "Iteration 41, loss = 0.28125264\n",
      "Iteration 42, loss = 0.27997168\n",
      "Iteration 43, loss = 0.27744942\n",
      "Iteration 44, loss = 0.27469913\n",
      "Iteration 45, loss = 0.27320842\n",
      "Iteration 46, loss = 0.27096273\n",
      "Iteration 47, loss = 0.26983690\n",
      "Iteration 48, loss = 0.26765654\n",
      "Iteration 49, loss = 0.26559137\n",
      "Iteration 50, loss = 0.26471866\n",
      "Iteration 51, loss = 0.26348719\n",
      "Iteration 52, loss = 0.26069170\n",
      "Iteration 53, loss = 0.26099742\n",
      "Iteration 54, loss = 0.25945305\n",
      "Iteration 55, loss = 0.25789494\n",
      "Iteration 56, loss = 0.25617978\n",
      "Iteration 57, loss = 0.25438760\n",
      "Iteration 58, loss = 0.25374307\n",
      "Iteration 59, loss = 0.25211356\n",
      "Iteration 60, loss = 0.25116291\n",
      "Iteration 61, loss = 0.24933347\n",
      "Iteration 62, loss = 0.24926844\n",
      "Iteration 63, loss = 0.24792395\n",
      "Iteration 64, loss = 0.24749976\n",
      "Iteration 65, loss = 0.24608942\n",
      "Iteration 66, loss = 0.24476300\n",
      "Iteration 67, loss = 0.24272821\n",
      "Iteration 68, loss = 0.24259824\n",
      "Iteration 69, loss = 0.24177553\n",
      "Iteration 70, loss = 0.23968236\n",
      "Iteration 71, loss = 0.23919786\n",
      "Iteration 72, loss = 0.23927091\n",
      "Iteration 73, loss = 0.23803526\n",
      "Iteration 74, loss = 0.23636277\n",
      "Iteration 75, loss = 0.23617382\n",
      "Iteration 76, loss = 0.23510495\n",
      "Iteration 77, loss = 0.23435737\n",
      "Iteration 78, loss = 0.23399499\n",
      "Iteration 79, loss = 0.23271847\n",
      "Iteration 80, loss = 0.23203051\n",
      "Iteration 81, loss = 0.23077092\n",
      "Iteration 82, loss = 0.22992295\n",
      "Iteration 83, loss = 0.22941712\n",
      "Iteration 84, loss = 0.22868224\n",
      "Iteration 85, loss = 0.22848480\n",
      "Iteration 86, loss = 0.22753600\n",
      "Iteration 87, loss = 0.22673993\n",
      "Iteration 88, loss = 0.22590747\n",
      "Iteration 89, loss = 0.22505445\n",
      "Iteration 90, loss = 0.22497081\n",
      "Iteration 91, loss = 0.22430192\n",
      "Iteration 92, loss = 0.22409604\n",
      "Iteration 93, loss = 0.22267717\n",
      "Iteration 94, loss = 0.22278420\n",
      "Iteration 95, loss = 0.22158850\n",
      "Iteration 96, loss = 0.22003032\n",
      "Iteration 97, loss = 0.22073256\n",
      "Iteration 98, loss = 0.21915479\n",
      "Iteration 99, loss = 0.21869320\n",
      "Iteration 100, loss = 0.21762078\n",
      "Iteration 101, loss = 0.21777426\n",
      "Iteration 102, loss = 0.21758886\n",
      "Iteration 103, loss = 0.21786464\n",
      "Iteration 104, loss = 0.21702714\n",
      "Iteration 105, loss = 0.21568247\n",
      "Iteration 106, loss = 0.21517310\n",
      "Iteration 107, loss = 0.21398496\n",
      "Iteration 108, loss = 0.21466505\n",
      "Iteration 109, loss = 0.21287267\n",
      "Iteration 110, loss = 0.21239357\n",
      "Iteration 111, loss = 0.21216190\n",
      "Iteration 112, loss = 0.21151629\n",
      "Iteration 113, loss = 0.21190322\n",
      "Iteration 114, loss = 0.21117215\n",
      "Iteration 115, loss = 0.21057902\n",
      "Iteration 116, loss = 0.20998634\n",
      "Iteration 117, loss = 0.20850363\n",
      "Iteration 118, loss = 0.20909964\n",
      "Iteration 119, loss = 0.20824484\n",
      "Iteration 120, loss = 0.20758827\n",
      "Iteration 121, loss = 0.20707555\n",
      "Iteration 122, loss = 0.20624438\n",
      "Iteration 123, loss = 0.20674457\n",
      "Iteration 124, loss = 0.20548840\n",
      "Iteration 125, loss = 0.20593437\n",
      "Iteration 126, loss = 0.20623862\n",
      "Iteration 127, loss = 0.20416989\n",
      "Iteration 128, loss = 0.20370298\n",
      "Iteration 129, loss = 0.20237522\n",
      "Iteration 130, loss = 0.20304736\n",
      "Iteration 131, loss = 0.20271616\n",
      "Iteration 132, loss = 0.20162080\n",
      "Iteration 133, loss = 0.20132993\n",
      "Iteration 134, loss = 0.20176065\n",
      "Iteration 135, loss = 0.20191275\n",
      "Iteration 136, loss = 0.20086930\n",
      "Iteration 137, loss = 0.19940440\n",
      "Iteration 138, loss = 0.19918227\n",
      "Iteration 139, loss = 0.19809298\n",
      "Iteration 140, loss = 0.19950532\n",
      "Iteration 141, loss = 0.19913190\n",
      "Iteration 142, loss = 0.19888358\n",
      "Iteration 143, loss = 0.19830642\n",
      "Iteration 144, loss = 0.19707679\n",
      "Iteration 145, loss = 0.19709472\n",
      "Iteration 146, loss = 0.19581871\n",
      "Iteration 147, loss = 0.19611713\n",
      "Iteration 148, loss = 0.19472505\n",
      "Iteration 149, loss = 0.19486225\n",
      "Iteration 150, loss = 0.19555437\n",
      "Iteration 151, loss = 0.19499541\n",
      "Iteration 152, loss = 0.19551164\n",
      "Iteration 153, loss = 0.19324414\n",
      "Iteration 154, loss = 0.19368552\n",
      "Iteration 155, loss = 0.19423389\n",
      "Iteration 156, loss = 0.19375917\n",
      "Iteration 157, loss = 0.19290962\n",
      "Iteration 158, loss = 0.19214102\n",
      "Iteration 159, loss = 0.19255021\n",
      "Iteration 160, loss = 0.19335606\n",
      "Iteration 161, loss = 0.18996082\n",
      "Iteration 162, loss = 0.19103262\n",
      "Iteration 163, loss = 0.19091739\n",
      "Iteration 164, loss = 0.19063997\n",
      "Iteration 165, loss = 0.19037365\n",
      "Iteration 166, loss = 0.19029520\n",
      "Iteration 167, loss = 0.18940856\n",
      "Iteration 168, loss = 0.18952952\n",
      "Iteration 169, loss = 0.18909257\n",
      "Iteration 170, loss = 0.18793709\n",
      "Iteration 171, loss = 0.18732380\n",
      "Iteration 172, loss = 0.18894534\n",
      "Iteration 173, loss = 0.18790631\n",
      "Iteration 174, loss = 0.18684378\n",
      "Iteration 175, loss = 0.18583217\n",
      "Iteration 176, loss = 0.18616285\n",
      "Iteration 177, loss = 0.18527423\n",
      "Iteration 178, loss = 0.18700140\n",
      "Iteration 179, loss = 0.18711370\n",
      "Iteration 180, loss = 0.18457599\n",
      "Iteration 181, loss = 0.18440537\n",
      "Iteration 182, loss = 0.18438414\n",
      "Iteration 183, loss = 0.18364971\n",
      "Iteration 184, loss = 0.18479765\n",
      "Iteration 185, loss = 0.18464376\n",
      "Iteration 186, loss = 0.18411985\n",
      "Iteration 187, loss = 0.18445738\n",
      "Iteration 188, loss = 0.18363946\n",
      "Iteration 189, loss = 0.18328554\n",
      "Iteration 190, loss = 0.18327131\n",
      "Iteration 191, loss = 0.18236118\n",
      "Iteration 192, loss = 0.18175295\n",
      "Iteration 193, loss = 0.18274488\n",
      "Iteration 194, loss = 0.18195434\n",
      "Iteration 195, loss = 0.18158208\n",
      "Iteration 196, loss = 0.18069347\n",
      "Iteration 197, loss = 0.18089073\n",
      "Iteration 198, loss = 0.18026396\n",
      "Iteration 199, loss = 0.18070802\n",
      "Iteration 200, loss = 0.17998776\n",
      "Iteration 201, loss = 0.17955864\n",
      "Iteration 202, loss = 0.18050986\n",
      "Iteration 203, loss = 0.17955116\n",
      "Iteration 204, loss = 0.17893336\n",
      "Iteration 205, loss = 0.17924055\n",
      "Iteration 206, loss = 0.17800996\n",
      "Iteration 207, loss = 0.17814119\n",
      "Iteration 208, loss = 0.17895125\n",
      "Iteration 209, loss = 0.17761115\n",
      "Iteration 210, loss = 0.17777011\n",
      "Iteration 211, loss = 0.17772122\n",
      "Iteration 212, loss = 0.17814588\n",
      "Iteration 213, loss = 0.17716227\n",
      "Iteration 214, loss = 0.17768826\n",
      "Iteration 215, loss = 0.17612025\n",
      "Iteration 216, loss = 0.17633930\n",
      "Iteration 217, loss = 0.17774947\n",
      "Iteration 218, loss = 0.17786594\n",
      "Iteration 219, loss = 0.17527478\n",
      "Iteration 220, loss = 0.17646474\n",
      "Iteration 221, loss = 0.17566849\n",
      "Iteration 222, loss = 0.17681542\n",
      "Iteration 223, loss = 0.17588569\n",
      "Iteration 224, loss = 0.17451310\n",
      "Iteration 225, loss = 0.17435719\n",
      "Iteration 226, loss = 0.17517988\n",
      "Iteration 227, loss = 0.17374785\n",
      "Iteration 228, loss = 0.17406307\n",
      "Iteration 229, loss = 0.17402743\n",
      "Iteration 230, loss = 0.17508296\n",
      "Iteration 231, loss = 0.17172859\n",
      "Iteration 232, loss = 0.17318924\n",
      "Iteration 233, loss = 0.17234690\n",
      "Iteration 234, loss = 0.17296417\n",
      "Iteration 235, loss = 0.17252591\n",
      "Iteration 236, loss = 0.17445575\n",
      "Iteration 237, loss = 0.17174349\n",
      "Iteration 238, loss = 0.17201452\n",
      "Iteration 239, loss = 0.17375125\n",
      "Iteration 240, loss = 0.17190038\n",
      "Iteration 241, loss = 0.17029434\n",
      "Iteration 242, loss = 0.17108907\n",
      "Iteration 243, loss = 0.17020447\n",
      "Iteration 244, loss = 0.16989186\n",
      "Iteration 245, loss = 0.17198008\n",
      "Iteration 246, loss = 0.17005931\n",
      "Iteration 247, loss = 0.16946048\n",
      "Iteration 248, loss = 0.17021625\n",
      "Iteration 249, loss = 0.16936473\n",
      "Iteration 250, loss = 0.16942907\n",
      "Iteration 251, loss = 0.16835789\n",
      "Iteration 252, loss = 0.16904284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.16802667\n",
      "Iteration 254, loss = 0.16940432\n",
      "Iteration 255, loss = 0.16838774\n",
      "Iteration 256, loss = 0.16864312\n",
      "Iteration 257, loss = 0.16821733\n",
      "Iteration 258, loss = 0.16852126\n",
      "Iteration 259, loss = 0.16882121\n",
      "Iteration 260, loss = 0.16727942\n",
      "Iteration 261, loss = 0.16815847\n",
      "Iteration 262, loss = 0.16790269\n",
      "Iteration 263, loss = 0.16655166\n",
      "Iteration 264, loss = 0.16784392\n",
      "Iteration 265, loss = 0.16708271\n",
      "Iteration 266, loss = 0.16604344\n",
      "Iteration 267, loss = 0.16693701\n",
      "Iteration 268, loss = 0.16662961\n",
      "Iteration 269, loss = 0.16597191\n",
      "Iteration 270, loss = 0.16607054\n",
      "Iteration 271, loss = 0.16514548\n",
      "Iteration 272, loss = 0.16547627\n",
      "Iteration 273, loss = 0.16593260\n",
      "Iteration 274, loss = 0.16477371\n",
      "Iteration 275, loss = 0.16315331\n",
      "Iteration 276, loss = 0.16507616\n",
      "Iteration 277, loss = 0.16371813\n",
      "Iteration 278, loss = 0.16486294\n",
      "Iteration 279, loss = 0.16464084\n",
      "Iteration 280, loss = 0.16494452\n",
      "Iteration 281, loss = 0.16298799\n",
      "Iteration 282, loss = 0.16590439\n",
      "Iteration 283, loss = 0.16254029\n",
      "Iteration 284, loss = 0.16348627\n",
      "Iteration 285, loss = 0.16216248\n",
      "Iteration 286, loss = 0.16256260\n",
      "Iteration 287, loss = 0.16200627\n",
      "Iteration 288, loss = 0.16223989\n",
      "Iteration 289, loss = 0.16264211\n",
      "Iteration 290, loss = 0.16245779\n",
      "Iteration 291, loss = 0.16109565\n",
      "Iteration 292, loss = 0.16236851\n",
      "Iteration 293, loss = 0.16116823\n",
      "Iteration 294, loss = 0.16234993\n",
      "Iteration 295, loss = 0.16124899\n",
      "Iteration 296, loss = 0.16383512\n",
      "Iteration 297, loss = 0.16265313\n",
      "Iteration 298, loss = 0.16181492\n",
      "Iteration 299, loss = 0.16109023\n",
      "Iteration 300, loss = 0.16040440\n",
      "Iteration 301, loss = 0.15921426\n",
      "Iteration 302, loss = 0.15903412\n",
      "Iteration 303, loss = 0.16015446\n",
      "Iteration 304, loss = 0.16033949\n",
      "Iteration 305, loss = 0.15932389\n",
      "Iteration 306, loss = 0.15877114\n",
      "Iteration 307, loss = 0.16001323\n",
      "Iteration 308, loss = 0.15880026\n",
      "Iteration 309, loss = 0.15872047\n",
      "Iteration 310, loss = 0.15898917\n",
      "Iteration 311, loss = 0.15929170\n",
      "Iteration 312, loss = 0.15813261\n",
      "Iteration 313, loss = 0.15928416\n",
      "Iteration 314, loss = 0.15785943\n",
      "Iteration 315, loss = 0.15845741\n",
      "Iteration 316, loss = 0.15803577\n",
      "Iteration 317, loss = 0.15861054\n",
      "Iteration 318, loss = 0.15895481\n",
      "Iteration 319, loss = 0.15754935\n",
      "Iteration 320, loss = 0.15783019\n",
      "Iteration 321, loss = 0.15790459\n",
      "Iteration 322, loss = 0.15720870\n",
      "Iteration 323, loss = 0.15642390\n",
      "Iteration 324, loss = 0.15698849\n",
      "Iteration 325, loss = 0.15628828\n",
      "Iteration 326, loss = 0.15626473\n",
      "Iteration 327, loss = 0.15611860\n",
      "Iteration 328, loss = 0.15644911\n",
      "Iteration 329, loss = 0.15544882\n",
      "Iteration 330, loss = 0.15551932\n",
      "Iteration 331, loss = 0.15669394\n",
      "Iteration 332, loss = 0.15508508\n",
      "Iteration 333, loss = 0.15614674\n",
      "Iteration 334, loss = 0.15806983\n",
      "Iteration 335, loss = 0.15550668\n",
      "Iteration 336, loss = 0.15627136\n",
      "Iteration 337, loss = 0.15516436\n",
      "Iteration 338, loss = 0.15494032\n",
      "Iteration 339, loss = 0.15518390\n",
      "Iteration 340, loss = 0.15494990\n",
      "Iteration 341, loss = 0.15379144\n",
      "Iteration 342, loss = 0.15506213\n",
      "Iteration 343, loss = 0.15465518\n",
      "Iteration 344, loss = 0.15352371\n",
      "Iteration 345, loss = 0.15548140\n",
      "Iteration 346, loss = 0.15325964\n",
      "Iteration 347, loss = 0.15352320\n",
      "Iteration 348, loss = 0.15343452\n",
      "Iteration 349, loss = 0.15239510\n",
      "Iteration 350, loss = 0.15504211\n",
      "Iteration 351, loss = 0.15345889\n",
      "Iteration 352, loss = 0.15372748\n",
      "Iteration 353, loss = 0.15326311\n",
      "Iteration 354, loss = 0.15258683\n",
      "Iteration 355, loss = 0.15344929\n",
      "Iteration 356, loss = 0.15376168\n",
      "Iteration 357, loss = 0.15193176\n",
      "Iteration 358, loss = 0.15105638\n",
      "Iteration 359, loss = 0.15247034\n",
      "Iteration 360, loss = 0.15152809\n",
      "Iteration 361, loss = 0.15136092\n",
      "Iteration 362, loss = 0.15277133\n",
      "Iteration 363, loss = 0.15168302\n",
      "Iteration 364, loss = 0.15077304\n",
      "Iteration 365, loss = 0.15086469\n",
      "Iteration 366, loss = 0.15117941\n",
      "Iteration 367, loss = 0.15151283\n",
      "Iteration 368, loss = 0.15139774\n",
      "Iteration 369, loss = 0.15012195\n",
      "Iteration 370, loss = 0.15106312\n",
      "Iteration 371, loss = 0.14972840\n",
      "Iteration 372, loss = 0.15006523\n",
      "Iteration 373, loss = 0.15001211\n",
      "Iteration 374, loss = 0.15036689\n",
      "Iteration 375, loss = 0.14979976\n",
      "Iteration 376, loss = 0.14914264\n",
      "Iteration 377, loss = 0.15114576\n",
      "Iteration 378, loss = 0.15059261\n",
      "Iteration 379, loss = 0.15079837\n",
      "Iteration 380, loss = 0.14931201\n",
      "Iteration 381, loss = 0.15106621\n",
      "Iteration 382, loss = 0.14890967\n",
      "Iteration 383, loss = 0.14958159\n",
      "Iteration 384, loss = 0.14934382\n",
      "Iteration 385, loss = 0.14807987\n",
      "Iteration 386, loss = 0.14862200\n",
      "Iteration 387, loss = 0.14968858\n",
      "Iteration 388, loss = 0.14913418\n",
      "Iteration 389, loss = 0.15072120\n",
      "Iteration 390, loss = 0.14862996\n",
      "Iteration 391, loss = 0.14787190\n",
      "Iteration 392, loss = 0.14884802\n",
      "Iteration 393, loss = 0.14851558\n",
      "Iteration 394, loss = 0.14683016\n",
      "Iteration 395, loss = 0.14781701\n",
      "Iteration 396, loss = 0.14816179\n",
      "Iteration 397, loss = 0.14761684\n",
      "Iteration 398, loss = 0.14627280\n",
      "Iteration 399, loss = 0.14718294\n",
      "Iteration 400, loss = 0.14690615\n",
      "Iteration 401, loss = 0.14656111\n",
      "Iteration 402, loss = 0.14623522\n",
      "Iteration 403, loss = 0.14515909\n",
      "Iteration 404, loss = 0.14676273\n",
      "Iteration 405, loss = 0.14778699\n",
      "Iteration 406, loss = 0.14675822\n",
      "Iteration 407, loss = 0.14630447\n",
      "Iteration 408, loss = 0.14572523\n",
      "Iteration 409, loss = 0.14749577\n",
      "Iteration 410, loss = 0.14683333\n",
      "Iteration 411, loss = 0.14556414\n",
      "Iteration 412, loss = 0.14644480\n",
      "Iteration 413, loss = 0.14502374\n",
      "Iteration 414, loss = 0.14616171\n",
      "Iteration 415, loss = 0.14539687\n",
      "Iteration 416, loss = 0.14507180\n",
      "Iteration 417, loss = 0.14552701\n",
      "Iteration 418, loss = 0.14467241\n",
      "Iteration 419, loss = 0.14620958\n",
      "Iteration 420, loss = 0.14457455\n",
      "Iteration 421, loss = 0.14510958\n",
      "Iteration 422, loss = 0.14474965\n",
      "Iteration 423, loss = 0.14439163\n",
      "Iteration 424, loss = 0.14648419\n",
      "Iteration 425, loss = 0.14498605\n",
      "Iteration 426, loss = 0.14400960\n",
      "Iteration 427, loss = 0.14399227\n",
      "Iteration 428, loss = 0.14403078\n",
      "Iteration 429, loss = 0.14338859\n",
      "Iteration 430, loss = 0.14318698\n",
      "Iteration 431, loss = 0.14339183\n",
      "Iteration 432, loss = 0.14557683\n",
      "Iteration 433, loss = 0.14348761\n",
      "Iteration 434, loss = 0.14290809\n",
      "Iteration 435, loss = 0.14415534\n",
      "Iteration 436, loss = 0.14330074\n",
      "Iteration 437, loss = 0.14355428\n",
      "Iteration 438, loss = 0.14265234\n",
      "Iteration 439, loss = 0.14221499\n",
      "Iteration 440, loss = 0.14338851\n",
      "Iteration 441, loss = 0.14297539\n",
      "Iteration 442, loss = 0.14258847\n",
      "Iteration 443, loss = 0.14162502\n",
      "Iteration 444, loss = 0.14310943\n",
      "Iteration 445, loss = 0.14253392\n",
      "Iteration 446, loss = 0.14044237\n",
      "Iteration 447, loss = 0.14170735\n",
      "Iteration 448, loss = 0.14230422\n",
      "Iteration 449, loss = 0.14277541\n",
      "Iteration 450, loss = 0.14161393\n",
      "Iteration 451, loss = 0.14167140\n",
      "Iteration 452, loss = 0.14081992\n",
      "Iteration 453, loss = 0.14180561\n",
      "Iteration 454, loss = 0.14110831\n",
      "Iteration 455, loss = 0.14170567\n",
      "Iteration 456, loss = 0.14141437\n",
      "Iteration 457, loss = 0.13992933\n",
      "Iteration 458, loss = 0.14087099\n",
      "Iteration 459, loss = 0.13979553\n",
      "Iteration 460, loss = 0.14047514\n",
      "Iteration 461, loss = 0.13961835\n",
      "Iteration 462, loss = 0.13977097\n",
      "Iteration 463, loss = 0.13893977\n",
      "Iteration 464, loss = 0.14004366\n",
      "Iteration 465, loss = 0.14020877\n",
      "Iteration 466, loss = 0.13923044\n",
      "Iteration 467, loss = 0.13989349\n",
      "Iteration 468, loss = 0.13986700\n",
      "Iteration 469, loss = 0.13949228\n",
      "Iteration 470, loss = 0.13976295\n",
      "Iteration 471, loss = 0.13927520\n",
      "Iteration 472, loss = 0.14042743\n",
      "Iteration 473, loss = 0.14093153\n",
      "Iteration 474, loss = 0.14042251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, verbose=True)\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)\n",
    "\n",
    "# default loss function is cross-entropy or log-loss \n",
    "# classifier predicts a probability (0-1), while the actual is 0 or 1 see doc below\n",
    "# https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_pred = mlp_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2514,   56,    0,   22,  105],\n",
       "       [  51, 1887,    0,    8,    0],\n",
       "       [   0,    0,    0,    0,    1],\n",
       "       [  27,   14,    0,  588,   17],\n",
       "       [ 117,    0,    0,   13,  710]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9296900489396411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12854760\n",
      "Iteration 2, loss = 0.77399832\n",
      "Iteration 3, loss = 0.67038908\n",
      "Iteration 4, loss = 0.60814202\n",
      "Iteration 5, loss = 0.56114792\n",
      "Iteration 6, loss = 0.52491431\n",
      "Iteration 7, loss = 0.49265161\n",
      "Iteration 8, loss = 0.46366996\n",
      "Iteration 9, loss = 0.44012949\n",
      "Iteration 10, loss = 0.41860469\n",
      "Iteration 11, loss = 0.40049457\n",
      "Iteration 12, loss = 0.38586595\n",
      "Iteration 13, loss = 0.37177235\n",
      "Iteration 14, loss = 0.35997851\n",
      "Iteration 15, loss = 0.34785978\n",
      "Iteration 16, loss = 0.33944618\n",
      "Iteration 17, loss = 0.33106611\n",
      "Iteration 18, loss = 0.32400551\n",
      "Iteration 19, loss = 0.31716916\n",
      "Iteration 20, loss = 0.30915972\n",
      "Iteration 21, loss = 0.30481968\n",
      "Iteration 22, loss = 0.30005806\n",
      "Iteration 23, loss = 0.29439003\n",
      "Iteration 24, loss = 0.29073462\n",
      "Iteration 25, loss = 0.28578385\n",
      "Iteration 26, loss = 0.28256845\n",
      "Iteration 27, loss = 0.27924316\n",
      "Iteration 28, loss = 0.27550826\n",
      "Iteration 29, loss = 0.27180245\n",
      "Iteration 30, loss = 0.27072390\n",
      "Iteration 31, loss = 0.26793308\n",
      "Iteration 32, loss = 0.26496409\n",
      "Iteration 33, loss = 0.26219618\n",
      "Iteration 34, loss = 0.25963338\n",
      "Iteration 35, loss = 0.25667652\n",
      "Iteration 36, loss = 0.25622461\n",
      "Iteration 37, loss = 0.25239280\n",
      "Iteration 38, loss = 0.25169374\n",
      "Iteration 39, loss = 0.25121191\n",
      "Iteration 40, loss = 0.24906313\n",
      "Iteration 41, loss = 0.24683316\n",
      "Iteration 42, loss = 0.24547539\n",
      "Iteration 43, loss = 0.24323804\n",
      "Iteration 44, loss = 0.24113082\n",
      "Iteration 45, loss = 0.24000443\n",
      "Iteration 46, loss = 0.23857404\n",
      "Iteration 47, loss = 0.23813309\n",
      "Iteration 48, loss = 0.23618419\n",
      "Iteration 49, loss = 0.23462352\n",
      "Iteration 50, loss = 0.23351110\n",
      "Iteration 51, loss = 0.23344365\n",
      "Iteration 52, loss = 0.23111053\n",
      "Iteration 53, loss = 0.22883291\n",
      "Iteration 54, loss = 0.22867283\n",
      "Iteration 55, loss = 0.22721933\n",
      "Iteration 56, loss = 0.22519946\n",
      "Iteration 57, loss = 0.22599396\n",
      "Iteration 58, loss = 0.22505211\n",
      "Iteration 59, loss = 0.22396106\n",
      "Iteration 60, loss = 0.22256042\n",
      "Iteration 61, loss = 0.22203990\n",
      "Iteration 62, loss = 0.22306877\n",
      "Iteration 63, loss = 0.22018894\n",
      "Iteration 64, loss = 0.21848716\n",
      "Iteration 65, loss = 0.21795497\n",
      "Iteration 66, loss = 0.21738067\n",
      "Iteration 67, loss = 0.21773693\n",
      "Iteration 68, loss = 0.21505481\n",
      "Iteration 69, loss = 0.21549408\n",
      "Iteration 70, loss = 0.21511663\n",
      "Iteration 71, loss = 0.21314233\n",
      "Iteration 72, loss = 0.21155475\n",
      "Iteration 73, loss = 0.21087822\n",
      "Iteration 74, loss = 0.21061535\n",
      "Iteration 75, loss = 0.20890681\n",
      "Iteration 76, loss = 0.20866937\n",
      "Iteration 77, loss = 0.20851060\n",
      "Iteration 78, loss = 0.20761976\n",
      "Iteration 79, loss = 0.20704524\n",
      "Iteration 80, loss = 0.20781042\n",
      "Iteration 81, loss = 0.20463816\n",
      "Iteration 82, loss = 0.20567582\n",
      "Iteration 83, loss = 0.20402844\n",
      "Iteration 84, loss = 0.20571001\n",
      "Iteration 85, loss = 0.20352252\n",
      "Iteration 86, loss = 0.20427178\n",
      "Iteration 87, loss = 0.20187981\n",
      "Iteration 88, loss = 0.20056022\n",
      "Iteration 89, loss = 0.20075913\n",
      "Iteration 90, loss = 0.20106437\n",
      "Iteration 91, loss = 0.20186320\n",
      "Iteration 92, loss = 0.19942253\n",
      "Iteration 93, loss = 0.19894809\n",
      "Iteration 94, loss = 0.19937920\n",
      "Iteration 95, loss = 0.19653167\n",
      "Iteration 96, loss = 0.19644084\n",
      "Iteration 97, loss = 0.19587858\n",
      "Iteration 98, loss = 0.19614604\n",
      "Iteration 99, loss = 0.19484027\n",
      "Iteration 100, loss = 0.19351393\n",
      "Iteration 101, loss = 0.19540528\n",
      "Iteration 102, loss = 0.19411196\n",
      "Iteration 103, loss = 0.19479084\n",
      "Iteration 104, loss = 0.19404063\n",
      "Iteration 105, loss = 0.19153420\n",
      "Iteration 106, loss = 0.19161831\n",
      "Iteration 107, loss = 0.19323224\n",
      "Iteration 108, loss = 0.18963289\n",
      "Iteration 109, loss = 0.19123746\n",
      "Iteration 110, loss = 0.18940272\n",
      "Iteration 111, loss = 0.18971536\n",
      "Iteration 112, loss = 0.18943153\n",
      "Iteration 113, loss = 0.18848594\n",
      "Iteration 114, loss = 0.18846623\n",
      "Iteration 115, loss = 0.18654369\n",
      "Iteration 116, loss = 0.18803543\n",
      "Iteration 117, loss = 0.18680081\n",
      "Iteration 118, loss = 0.18628293\n",
      "Iteration 119, loss = 0.18656045\n",
      "Iteration 120, loss = 0.18494103\n",
      "Iteration 121, loss = 0.18643216\n",
      "Iteration 122, loss = 0.18601181\n",
      "Iteration 123, loss = 0.18609689\n",
      "Iteration 124, loss = 0.18335640\n",
      "Iteration 125, loss = 0.18384380\n",
      "Iteration 126, loss = 0.18272497\n",
      "Iteration 127, loss = 0.18318092\n",
      "Iteration 128, loss = 0.18252129\n",
      "Iteration 129, loss = 0.18376427\n",
      "Iteration 130, loss = 0.18517440\n",
      "Iteration 131, loss = 0.18202043\n",
      "Iteration 132, loss = 0.18217944\n",
      "Iteration 133, loss = 0.18121428\n",
      "Iteration 134, loss = 0.18059436\n",
      "Iteration 135, loss = 0.18190867\n",
      "Iteration 136, loss = 0.17885787\n",
      "Iteration 137, loss = 0.18062162\n",
      "Iteration 138, loss = 0.17978294\n",
      "Iteration 139, loss = 0.17887500\n",
      "Iteration 140, loss = 0.17794682\n",
      "Iteration 141, loss = 0.17807482\n",
      "Iteration 142, loss = 0.17918484\n",
      "Iteration 143, loss = 0.17748556\n",
      "Iteration 144, loss = 0.17742978\n",
      "Iteration 145, loss = 0.17741434\n",
      "Iteration 146, loss = 0.17523031\n",
      "Iteration 147, loss = 0.17564433\n",
      "Iteration 148, loss = 0.17554974\n",
      "Iteration 149, loss = 0.17488274\n",
      "Iteration 150, loss = 0.17483106\n",
      "Iteration 151, loss = 0.17600358\n",
      "Iteration 152, loss = 0.17434414\n",
      "Iteration 153, loss = 0.17414826\n",
      "Iteration 154, loss = 0.17275401\n",
      "Iteration 155, loss = 0.17382561\n",
      "Iteration 156, loss = 0.17390880\n",
      "Iteration 157, loss = 0.17308398\n",
      "Iteration 158, loss = 0.17291285\n",
      "Iteration 159, loss = 0.17049018\n",
      "Iteration 160, loss = 0.17186773\n",
      "Iteration 161, loss = 0.17222472\n",
      "Iteration 162, loss = 0.17210906\n",
      "Iteration 163, loss = 0.17040910\n",
      "Iteration 164, loss = 0.17133191\n",
      "Iteration 165, loss = 0.17044690\n",
      "Iteration 166, loss = 0.17046770\n",
      "Iteration 167, loss = 0.17266094\n",
      "Iteration 168, loss = 0.17034367\n",
      "Iteration 169, loss = 0.16913865\n",
      "Iteration 170, loss = 0.17078167\n",
      "Iteration 171, loss = 0.16958832\n",
      "Iteration 172, loss = 0.16946923\n",
      "Iteration 173, loss = 0.16968319\n",
      "Iteration 174, loss = 0.16760347\n",
      "Iteration 175, loss = 0.16902035\n",
      "Iteration 176, loss = 0.16639838\n",
      "Iteration 177, loss = 0.16797044\n",
      "Iteration 178, loss = 0.16718213\n",
      "Iteration 179, loss = 0.16746980\n",
      "Iteration 180, loss = 0.16634223\n",
      "Iteration 181, loss = 0.16713772\n",
      "Iteration 182, loss = 0.16612954\n",
      "Iteration 183, loss = 0.16866024\n",
      "Iteration 184, loss = 0.16599944\n",
      "Iteration 185, loss = 0.16511189\n",
      "Iteration 186, loss = 0.16506337\n",
      "Iteration 187, loss = 0.16739728\n",
      "Iteration 188, loss = 0.16323237\n",
      "Iteration 189, loss = 0.16309636\n",
      "Iteration 190, loss = 0.16258785\n",
      "Iteration 191, loss = 0.16294171\n",
      "Iteration 192, loss = 0.16236629\n",
      "Iteration 193, loss = 0.16191121\n",
      "Iteration 194, loss = 0.16251873\n",
      "Iteration 195, loss = 0.16285178\n",
      "Iteration 196, loss = 0.16126791\n",
      "Iteration 197, loss = 0.16138937\n",
      "Iteration 198, loss = 0.16178534\n",
      "Iteration 199, loss = 0.16165373\n",
      "Iteration 200, loss = 0.16132878\n",
      "Iteration 201, loss = 0.16247095\n",
      "Iteration 202, loss = 0.16126297\n",
      "Iteration 203, loss = 0.16195429\n",
      "Iteration 204, loss = 0.16011081\n",
      "Iteration 205, loss = 0.16072095\n",
      "Iteration 206, loss = 0.15888044\n",
      "Iteration 207, loss = 0.16053728\n",
      "Iteration 208, loss = 0.16036795\n",
      "Iteration 209, loss = 0.16028614\n",
      "Iteration 210, loss = 0.15869367\n",
      "Iteration 211, loss = 0.15957040\n",
      "Iteration 212, loss = 0.15819864\n",
      "Iteration 213, loss = 0.15839582\n",
      "Iteration 214, loss = 0.15822645\n",
      "Iteration 215, loss = 0.15813989\n",
      "Iteration 216, loss = 0.15663151\n",
      "Iteration 217, loss = 0.15884665\n",
      "Iteration 218, loss = 0.15638759\n",
      "Iteration 219, loss = 0.15888190\n",
      "Iteration 220, loss = 0.15737895\n",
      "Iteration 221, loss = 0.15659221\n",
      "Iteration 222, loss = 0.15618595\n",
      "Iteration 223, loss = 0.15521270\n",
      "Iteration 224, loss = 0.15542870\n",
      "Iteration 225, loss = 0.15638528\n",
      "Iteration 226, loss = 0.15590352\n",
      "Iteration 227, loss = 0.15453598\n",
      "Iteration 228, loss = 0.15573016\n",
      "Iteration 229, loss = 0.15565490\n",
      "Iteration 230, loss = 0.15461565\n",
      "Iteration 231, loss = 0.15421410\n",
      "Iteration 232, loss = 0.15537117\n",
      "Iteration 233, loss = 0.15378849\n",
      "Iteration 234, loss = 0.15492686\n",
      "Iteration 235, loss = 0.15433715\n",
      "Iteration 236, loss = 0.15477912\n",
      "Iteration 237, loss = 0.15384230\n",
      "Iteration 238, loss = 0.15237270\n",
      "Iteration 239, loss = 0.15300325\n",
      "Iteration 240, loss = 0.15211669\n",
      "Iteration 241, loss = 0.15322932\n",
      "Iteration 242, loss = 0.15271208\n",
      "Iteration 243, loss = 0.15163943\n",
      "Iteration 244, loss = 0.15213486\n",
      "Iteration 245, loss = 0.15114430\n",
      "Iteration 246, loss = 0.15054746\n",
      "Iteration 247, loss = 0.15127947\n",
      "Iteration 248, loss = 0.15243558\n",
      "Iteration 249, loss = 0.15004503\n",
      "Iteration 250, loss = 0.15202615\n",
      "Iteration 251, loss = 0.15128188\n",
      "Iteration 252, loss = 0.14944871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.15063938\n",
      "Iteration 254, loss = 0.14973490\n",
      "Iteration 255, loss = 0.15096268\n",
      "Iteration 256, loss = 0.15078043\n",
      "Iteration 257, loss = 0.15002422\n",
      "Iteration 258, loss = 0.14970016\n",
      "Iteration 259, loss = 0.14956942\n",
      "Iteration 260, loss = 0.14918171\n",
      "Iteration 261, loss = 0.15003590\n",
      "Iteration 262, loss = 0.14881563\n",
      "Iteration 263, loss = 0.14763150\n",
      "Iteration 264, loss = 0.14739326\n",
      "Iteration 265, loss = 0.14814241\n",
      "Iteration 266, loss = 0.14746554\n",
      "Iteration 267, loss = 0.14766647\n",
      "Iteration 268, loss = 0.14742928\n",
      "Iteration 269, loss = 0.14686761\n",
      "Iteration 270, loss = 0.14812879\n",
      "Iteration 271, loss = 0.14684088\n",
      "Iteration 272, loss = 0.14782916\n",
      "Iteration 273, loss = 0.14779705\n",
      "Iteration 274, loss = 0.14895877\n",
      "Iteration 275, loss = 0.14694685\n",
      "Iteration 276, loss = 0.14725432\n",
      "Iteration 277, loss = 0.14502272\n",
      "Iteration 278, loss = 0.14680433\n",
      "Iteration 279, loss = 0.14499509\n",
      "Iteration 280, loss = 0.14553298\n",
      "Iteration 281, loss = 0.14542675\n",
      "Iteration 282, loss = 0.14703198\n",
      "Iteration 283, loss = 0.14528440\n",
      "Iteration 284, loss = 0.14471738\n",
      "Iteration 285, loss = 0.14371253\n",
      "Iteration 286, loss = 0.14469216\n",
      "Iteration 287, loss = 0.14342313\n",
      "Iteration 288, loss = 0.14586186\n",
      "Iteration 289, loss = 0.14578049\n",
      "Iteration 290, loss = 0.14518316\n",
      "Iteration 291, loss = 0.14312618\n",
      "Iteration 292, loss = 0.14290465\n",
      "Iteration 293, loss = 0.14344940\n",
      "Iteration 294, loss = 0.14359920\n",
      "Iteration 295, loss = 0.14237335\n",
      "Iteration 296, loss = 0.14404593\n",
      "Iteration 297, loss = 0.14093916\n",
      "Iteration 298, loss = 0.14109840\n",
      "Iteration 299, loss = 0.14235131\n",
      "Iteration 300, loss = 0.14227018\n",
      "Iteration 301, loss = 0.14224560\n",
      "Iteration 302, loss = 0.14238045\n",
      "Iteration 303, loss = 0.14236666\n",
      "Iteration 304, loss = 0.14163823\n",
      "Iteration 305, loss = 0.14035166\n",
      "Iteration 306, loss = 0.14427151\n",
      "Iteration 307, loss = 0.14082334\n",
      "Iteration 308, loss = 0.14041911\n",
      "Iteration 309, loss = 0.14062606\n",
      "Iteration 310, loss = 0.13986007\n",
      "Iteration 311, loss = 0.14066879\n",
      "Iteration 312, loss = 0.13964276\n",
      "Iteration 313, loss = 0.13969207\n",
      "Iteration 314, loss = 0.13995131\n",
      "Iteration 315, loss = 0.13965883\n",
      "Iteration 316, loss = 0.14016748\n",
      "Iteration 317, loss = 0.14086927\n",
      "Iteration 318, loss = 0.14108116\n",
      "Iteration 319, loss = 0.13998295\n",
      "Iteration 320, loss = 0.14085091\n",
      "Iteration 321, loss = 0.13815534\n",
      "Iteration 322, loss = 0.14094536\n",
      "Iteration 323, loss = 0.13938414\n",
      "Iteration 324, loss = 0.13892203\n",
      "Iteration 325, loss = 0.13676582\n",
      "Iteration 326, loss = 0.13793966\n",
      "Iteration 327, loss = 0.13895429\n",
      "Iteration 328, loss = 0.13935867\n",
      "Iteration 329, loss = 0.14174565\n",
      "Iteration 330, loss = 0.13779788\n",
      "Iteration 331, loss = 0.13730591\n",
      "Iteration 332, loss = 0.13709241\n",
      "Iteration 333, loss = 0.13758183\n",
      "Iteration 334, loss = 0.13594522\n",
      "Iteration 335, loss = 0.13681388\n",
      "Iteration 336, loss = 0.13995341\n",
      "Iteration 337, loss = 0.13645687\n",
      "Iteration 338, loss = 0.13733494\n",
      "Iteration 339, loss = 0.13627717\n",
      "Iteration 340, loss = 0.13646021\n",
      "Iteration 341, loss = 0.13548327\n",
      "Iteration 342, loss = 0.13505106\n",
      "Iteration 343, loss = 0.13695297\n",
      "Iteration 344, loss = 0.13793006\n",
      "Iteration 345, loss = 0.13695722\n",
      "Iteration 346, loss = 0.13672962\n",
      "Iteration 347, loss = 0.13597771\n",
      "Iteration 348, loss = 0.13753932\n",
      "Iteration 349, loss = 0.13530340\n",
      "Iteration 350, loss = 0.13493163\n",
      "Iteration 351, loss = 0.13576745\n",
      "Iteration 352, loss = 0.13754761\n",
      "Iteration 353, loss = 0.13407223\n",
      "Iteration 354, loss = 0.13634386\n",
      "Iteration 355, loss = 0.13374470\n",
      "Iteration 356, loss = 0.13612421\n",
      "Iteration 357, loss = 0.13326479\n",
      "Iteration 358, loss = 0.13632583\n",
      "Iteration 359, loss = 0.13386833\n",
      "Iteration 360, loss = 0.13371082\n",
      "Iteration 361, loss = 0.13335798\n",
      "Iteration 362, loss = 0.13389507\n",
      "Iteration 363, loss = 0.13393497\n",
      "Iteration 364, loss = 0.13561426\n",
      "Iteration 365, loss = 0.13558436\n",
      "Iteration 366, loss = 0.13433174\n",
      "Iteration 367, loss = 0.13434035\n",
      "Iteration 368, loss = 0.13347866\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increase neurons from 100 to 200\n",
    "mlp_clf = MLPClassifier(max_iter=1000, verbose=True,\n",
    "                        hidden_layer_sizes=(200,))\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9270799347471452\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = mlp_clf.predict(test_x)\n",
    "\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 25, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of layers:\n",
    "dnn_clf.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9424143556280588\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "print(accuracy_score(test_y, test_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 80, 60, 40, 20, 10),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=1000,\n",
       "       momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "       power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "       tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(100,80,60,40,20,10),\n",
    "                       max_iter=1000)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of layers:\n",
    "dnn_clf.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9430668841761827\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 25, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       early_stopping=True)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9057096247960849\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "print(accuracy_score(test_y, test_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 25, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       activation = 'tanh')\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9419249592169657\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver (Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 25, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's use Stochastic Gradient Descent optimizer\n",
    "\n",
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       activation = 'tanh',\n",
    "                       solver='sgd')\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935562805872757\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.62424286, -0.88621699, -0.36795414,  1.58359431,  0.60784241,\n",
       "         0.72632106,  0.80192168,  3.29843656,  2.53945568]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select a random observation\n",
    "\n",
    "random = test_x[50:51]\n",
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.19219599e-02, 2.44832585e-06, 3.21519363e-04, 3.33386580e-06,\n",
       "        9.07750739e-01]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the input variables of the observation\n",
    "dnn_clf.predict_proba(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round the probability values\n",
    "np.round(dnn_clf.predict_proba(random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12a6cb240>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(dnn_clf.loss_curve_)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
