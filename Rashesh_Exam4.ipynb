{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc, roc_auc_score, roc_curve\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the MNIST dataset\n",
    "(X_train, _), (X_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test2 = train_test_split(X_train, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (12000, 28, 28)\n",
      "Testing data shape:  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape: \", X_train.shape)\n",
    "\n",
    "print(\"Testing data shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data_MLP shape:  (12000, 784)\n",
      "Testing data_MLP shape:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Shape the data\n",
    "\n",
    "X_train_MLP = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test_MLP = X_test.reshape((X_test.shape[0], -1))\n",
    "print(\"Training data_MLP shape: \", X_train_MLP.shape)\n",
    "print(\"Testing data_MLP shape: \", X_test_MLP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of consitancy, the complile and fit are same across the notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_ONE = 64\n",
    "ENCODING_SIZE = 32\n",
    "\n",
    "input_MLP = Input(shape=(INPUT_SIZE,))\n",
    "\n",
    "MLP_encode = Dense(HIDDEN_ONE, activation='relu')(input_MLP)\n",
    "\n",
    "MLP_encode = Dense(ENCODING_SIZE, activation='relu')(MLP_encode)\n",
    "\n",
    "MLP_decode = Dense(HIDDEN_ONE, activation='relu')(MLP_encode)\n",
    "MLP_decode = Dense(INPUT_SIZE, activation='relu')(MLP_decode)\n",
    "\n",
    "autoencoder_MLP_1 = Model(input_MLP, MLP_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/10\n",
      "9600/9600 [==============================] - 0s 42us/step - loss: 0.0539 - val_loss: 0.0339\n",
      "Epoch 2/10\n",
      "9600/9600 [==============================] - 0s 25us/step - loss: 0.0299 - val_loss: 0.0270\n",
      "Epoch 3/10\n",
      "9600/9600 [==============================] - 0s 24us/step - loss: 0.0254 - val_loss: 0.0240\n",
      "Epoch 4/10\n",
      "9600/9600 [==============================] - 0s 25us/step - loss: 0.0232 - val_loss: 0.0226\n",
      "Epoch 5/10\n",
      "9600/9600 [==============================] - 0s 24us/step - loss: 0.0220 - val_loss: 0.0218\n",
      "Epoch 6/10\n",
      "9600/9600 [==============================] - 0s 26us/step - loss: 0.0212 - val_loss: 0.0210\n",
      "Epoch 7/10\n",
      "9600/9600 [==============================] - 0s 24us/step - loss: 0.0206 - val_loss: 0.0205\n",
      "Epoch 8/10\n",
      "9600/9600 [==============================] - 0s 25us/step - loss: 0.0202 - val_loss: 0.0203\n",
      "Epoch 9/10\n",
      "9600/9600 [==============================] - 0s 24us/step - loss: 0.0198 - val_loss: 0.0200\n",
      "Epoch 10/10\n",
      "9600/9600 [==============================] - 0s 24us/step - loss: 0.0194 - val_loss: 0.0193\n"
     ]
    }
   ],
   "source": [
    "autoencoder_MLP_1.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "autoencoder_MLP_1.fit(X_train_MLP,\n",
    "                   X_train_MLP,\n",
    "                   epochs=10,\n",
    "                   batch_size = 64,\n",
    "                   shuffle=True,\n",
    "                   validation_split=0.2)\n",
    "\n",
    "decoded_MLP_1 = autoencoder_MLP_1.predict(X_test_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_ONE = 128\n",
    "HIDDEN_TWO = 64\n",
    "ENCODING_SIZE = 32\n",
    "\n",
    "input_MLP = Input(shape=(INPUT_SIZE,))\n",
    "\n",
    "MLP_encode = Dense(HIDDEN_ONE, activation='relu')(input_MLP)\n",
    "MLP_encode = Dense(HIDDEN_TWO, activation='relu')(MLP_encode)\n",
    "\n",
    "MLP_encode = Dense(ENCODING_SIZE, activation='relu')(MLP_encode)\n",
    "\n",
    "MLP_decode = Dense(HIDDEN_TWO, activation='relu')(MLP_encode)\n",
    "MLP_decode = Dense(HIDDEN_ONE, activation='relu')(MLP_decode)\n",
    "MLP_decode = Dense(INPUT_SIZE, activation='relu')(MLP_decode)\n",
    "\n",
    "autoencoder_MLP_2 = Model(input_MLP, MLP_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/10\n",
      "9600/9600 [==============================] - 1s 64us/step - loss: 0.0524 - val_loss: 0.0339\n",
      "Epoch 2/10\n",
      "9600/9600 [==============================] - 0s 41us/step - loss: 0.0300 - val_loss: 0.0274\n",
      "Epoch 3/10\n",
      "9600/9600 [==============================] - 0s 38us/step - loss: 0.0258 - val_loss: 0.0245\n",
      "Epoch 4/10\n",
      "9600/9600 [==============================] - 0s 39us/step - loss: 0.0236 - val_loss: 0.0231\n",
      "Epoch 5/10\n",
      "9600/9600 [==============================] - 0s 37us/step - loss: 0.0224 - val_loss: 0.0221\n",
      "Epoch 6/10\n",
      "9600/9600 [==============================] - 0s 39us/step - loss: 0.0214 - val_loss: 0.0213\n",
      "Epoch 7/10\n",
      "9600/9600 [==============================] - 0s 38us/step - loss: 0.0206 - val_loss: 0.0207\n",
      "Epoch 8/10\n",
      "9600/9600 [==============================] - 0s 37us/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 9/10\n",
      "9600/9600 [==============================] - 0s 38us/step - loss: 0.0195 - val_loss: 0.0197\n",
      "Epoch 10/10\n",
      "9600/9600 [==============================] - 0s 38us/step - loss: 0.0190 - val_loss: 0.0192\n"
     ]
    }
   ],
   "source": [
    "autoencoder_MLP_2.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "autoencoder_MLP_2.fit(X_train_MLP,\n",
    "                   X_train_MLP,\n",
    "                   epochs=10,\n",
    "                   batch_size=64,\n",
    "                   shuffle=True,\n",
    "                   validation_split=0.2)\n",
    "\n",
    "decoded_MLP_2 = autoencoder_MLP_2.predict(X_test_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_ONE = 256\n",
    "HIDDEN_TWO = 128\n",
    "HIDDEN_THREE = 64\n",
    "ENCODING_SIZE = 32\n",
    "\n",
    "input_MLP = Input(shape=(INPUT_SIZE,))\n",
    "MLP_encode = Dense(HIDDEN_ONE, activation='relu')(input_MLP)\n",
    "MLP_encode = Dense(HIDDEN_TWO, activation='relu')(MLP_encode)\n",
    "MLP_encode = Dense(HIDDEN_THREE, activation='relu')(MLP_encode)\n",
    "\n",
    "MLP_encode = Dense(ENCODING_SIZE, activation='relu')(MLP_encode)\n",
    "\n",
    "MLP_decode = Dense(HIDDEN_THREE, activation='relu')(MLP_encode)\n",
    "MLP_decode = Dense(HIDDEN_TWO, activation='relu')(MLP_decode)\n",
    "MLP_decode = Dense(HIDDEN_ONE, activation='relu')(MLP_decode)\n",
    "MLP_decode = Dense(INPUT_SIZE, activation='relu')(MLP_decode)\n",
    "autoencoder_MLP_3 = Model(input_MLP, MLP_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/10\n",
      "9600/9600 [==============================] - 1s 96us/step - loss: 0.0578 - val_loss: 0.0372\n",
      "Epoch 2/10\n",
      "9600/9600 [==============================] - 1s 62us/step - loss: 0.0323 - val_loss: 0.0293\n",
      "Epoch 3/10\n",
      "9600/9600 [==============================] - 1s 64us/step - loss: 0.0272 - val_loss: 0.0253\n",
      "Epoch 4/10\n",
      "9600/9600 [==============================] - 1s 63us/step - loss: 0.0241 - val_loss: 0.0235\n",
      "Epoch 5/10\n",
      "9600/9600 [==============================] - 1s 66us/step - loss: 0.0221 - val_loss: 0.0217\n",
      "Epoch 6/10\n",
      "9600/9600 [==============================] - 1s 69us/step - loss: 0.0207 - val_loss: 0.0206\n",
      "Epoch 7/10\n",
      "9600/9600 [==============================] - 1s 67us/step - loss: 0.0198 - val_loss: 0.0198\n",
      "Epoch 8/10\n",
      "9600/9600 [==============================] - 1s 68us/step - loss: 0.0190 - val_loss: 0.0191\n",
      "Epoch 9/10\n",
      "9600/9600 [==============================] - 1s 69us/step - loss: 0.0184 - val_loss: 0.0186\n",
      "Epoch 10/10\n",
      "9600/9600 [==============================] - 1s 69us/step - loss: 0.0179 - val_loss: 0.0185\n"
     ]
    }
   ],
   "source": [
    "autoencoder_MLP_3.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "autoencoder_MLP_3.fit(X_train_MLP,\n",
    "                   X_train_MLP,\n",
    "                   epochs=10,\n",
    "                   batch_size=64,\n",
    "                   shuffle=True,\n",
    "                   validation_split=0.2)\n",
    "\n",
    "decoded_MLP_3 = autoencoder_MLP_3.predict(X_test_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Hidden Layer With Chanegs\n",
    "  \n",
    "# Increasing the size of the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_ONE = 512 #increasing the size of the layer\n",
    "HIDDEN_TWO = 256 #increasing the size of the layer\n",
    "ENCODING_SIZE = 32\n",
    "\n",
    "input_MLP = Input(shape=(INPUT_SIZE,))\n",
    "\n",
    "MLP_encode = Dense(HIDDEN_ONE, activation='relu')(input_MLP)\n",
    "MLP_encode = Dense(HIDDEN_TWO, activation='relu')(MLP_encode)\n",
    "\n",
    "MLP_encode = Dense(ENCODING_SIZE, activation='relu')(MLP_encode)\n",
    "\n",
    "MLP_decode = Dense(HIDDEN_TWO, activation='relu')(MLP_encode)\n",
    "MLP_decode = Dense(HIDDEN_ONE, activation='relu')(MLP_decode)\n",
    "MLP_decode = Dense(INPUT_SIZE, activation='relu')(MLP_decode)\n",
    "\n",
    "autoencoder_MLP_2_Change = Model(input_MLP, MLP_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/10\n",
      "9600/9600 [==============================] - 2s 162us/step - loss: 0.0412 - val_loss: 0.0255\n",
      "Epoch 2/10\n",
      "9600/9600 [==============================] - 1s 130us/step - loss: 0.0229 - val_loss: 0.0212\n",
      "Epoch 3/10\n",
      "9600/9600 [==============================] - 1s 137us/step - loss: 0.0195 - val_loss: 0.0188\n",
      "Epoch 4/10\n",
      "9600/9600 [==============================] - 1s 142us/step - loss: 0.0178 - val_loss: 0.0178\n",
      "Epoch 5/10\n",
      "9600/9600 [==============================] - 1s 143us/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 6/10\n",
      "9600/9600 [==============================] - 1s 142us/step - loss: 0.0159 - val_loss: 0.0163\n",
      "Epoch 7/10\n",
      "9600/9600 [==============================] - 1s 136us/step - loss: 0.0153 - val_loss: 0.0160\n",
      "Epoch 8/10\n",
      "9600/9600 [==============================] - 1s 144us/step - loss: 0.0148 - val_loss: 0.0156\n",
      "Epoch 9/10\n",
      "9600/9600 [==============================] - 1s 148us/step - loss: 0.0144 - val_loss: 0.0153\n",
      "Epoch 10/10\n",
      "9600/9600 [==============================] - 1s 149us/step - loss: 0.0141 - val_loss: 0.0149\n"
     ]
    }
   ],
   "source": [
    "autoencoder_MLP_2_Change.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "autoencoder_MLP_2_Change.fit(X_train_MLP,\n",
    "                   X_train_MLP,\n",
    "                   epochs=10,\n",
    "                   batch_size=64,\n",
    "                   shuffle=True,\n",
    "                   validation_split=0.2)\n",
    "\n",
    "decoded_MLP_2_Changes = autoencoder_MLP_2_Change.predict(X_test_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs \n",
    "\n",
    "    One Hidden Layer      loss: 0.0194 - val_loss: 0.0193\n",
    "  \n",
    "    Two Hidden Layer       loss: 0.0190 - val_loss: 0.0192\n",
    "      \n",
    "    Three Hidden Layer     oss: 0.0179 - val_loss: 0.0185\n",
    "  \n",
    "    Two Hidden Layer With Chanegs   loss: 0.0141 - val_loss: 0.0149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that, either increasing the number of hidden layers or the number of nodes in a layer, accuracy improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quesiton 2\n",
    "# Build any two CNN architectures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10 # 0-9 \n",
    "epochs = 3\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (12000, 28, 28, 1)\n",
      "12000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, ignore, y_train, ignore2 = train_test_split(x_train, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.6547 - accuracy: 0.7924 - val_loss: 0.1655 - val_accuracy: 0.9533\n",
      "Epoch 2/3\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.2000 - accuracy: 0.9385 - val_loss: 0.1128 - val_accuracy: 0.9636\n",
      "Epoch 3/3\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.1357 - accuracy: 0.9588 - val_loss: 0.0741 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a41130a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07407280763722957\n",
      "Test accuracy: 0.975600004196167\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "#if loaded_model:\n",
    "#    model = loaded_model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes made for Model 2:\n",
    "# Reversed the size of Layers \n",
    "# Removed The dropout layers\n",
    "# Changed Kernel_size \n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(128, kernel_size=(4, 4),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "\n",
    "model2.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "12000/12000 [==============================] - 27s 2ms/step - loss: 0.6803 - accuracy: 0.7881 - val_loss: 0.1858 - val_accuracy: 0.9422\n",
      "Epoch 2/3\n",
      "12000/12000 [==============================] - 26s 2ms/step - loss: 0.1615 - accuracy: 0.9534 - val_loss: 0.1127 - val_accuracy: 0.9623\n",
      "Epoch 3/3\n",
      "12000/12000 [==============================] - 26s 2ms/step - loss: 0.0899 - accuracy: 0.9726 - val_loss: 0.0978 - val_accuracy: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4277cf98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09783693803958594\n",
      "Test accuracy: 0.9692000150680542\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1:  Test accuracy: 0.9746999740600586  \n",
    "Model 2: Test accuracy: 0.9732000231742859\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that removing the dropout layers does affect the Accuracy of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
