{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dengue_features_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>prec_amt_mm</th>\n",
       "      <th>re_an_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>re_an_precip_amt_kg_per_m2</th>\n",
       "      <th>re_an_relative_hd_percent</th>\n",
       "      <th>re_an_sat_precip_amt_mm</th>\n",
       "      <th>re_an_specific_hd_g_per_kg</th>\n",
       "      <th>re_an_tdtr_k</th>\n",
       "      <th>stn_avg_temp_c</th>\n",
       "      <th>stn_diur_temp_rng_c</th>\n",
       "      <th>stn_max_temp_c</th>\n",
       "      <th>stn_min_temp_c</th>\n",
       "      <th>stn_precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>1990-04-30</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.198483</td>\n",
       "      <td>0.177617</td>\n",
       "      <td>12.42</td>\n",
       "      <td>297.572857</td>\n",
       "      <td>...</td>\n",
       "      <td>32.00</td>\n",
       "      <td>73.365714</td>\n",
       "      <td>12.42</td>\n",
       "      <td>14.012857</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>25.442857</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>29.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>1990-05-07</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.142175</td>\n",
       "      <td>0.162357</td>\n",
       "      <td>0.155486</td>\n",
       "      <td>22.82</td>\n",
       "      <td>298.211429</td>\n",
       "      <td>...</td>\n",
       "      <td>17.94</td>\n",
       "      <td>77.368571</td>\n",
       "      <td>22.82</td>\n",
       "      <td>15.372857</td>\n",
       "      <td>2.371429</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.371429</td>\n",
       "      <td>31.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>1990-05-14</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>34.54</td>\n",
       "      <td>298.781429</td>\n",
       "      <td>...</td>\n",
       "      <td>26.10</td>\n",
       "      <td>82.052857</td>\n",
       "      <td>34.54</td>\n",
       "      <td>16.848571</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.485714</td>\n",
       "      <td>32.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>1990-05-21</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>0.245067</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.235886</td>\n",
       "      <td>15.36</td>\n",
       "      <td>298.987143</td>\n",
       "      <td>...</td>\n",
       "      <td>13.90</td>\n",
       "      <td>80.337143</td>\n",
       "      <td>15.36</td>\n",
       "      <td>16.672857</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>27.471429</td>\n",
       "      <td>6.771429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>1990-05-28</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.247340</td>\n",
       "      <td>7.52</td>\n",
       "      <td>299.518571</td>\n",
       "      <td>...</td>\n",
       "      <td>12.20</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>7.52</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>3.014286</td>\n",
       "      <td>28.942857</td>\n",
       "      <td>9.371429</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>21</td>\n",
       "      <td>2010-05-28</td>\n",
       "      <td>0.342750</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.256343</td>\n",
       "      <td>0.292514</td>\n",
       "      <td>55.30</td>\n",
       "      <td>299.334286</td>\n",
       "      <td>...</td>\n",
       "      <td>45.00</td>\n",
       "      <td>88.765714</td>\n",
       "      <td>55.30</td>\n",
       "      <td>18.485714</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>28.633333</td>\n",
       "      <td>11.933333</td>\n",
       "      <td>35.4</td>\n",
       "      <td>22.4</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>22</td>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>0.160157</td>\n",
       "      <td>0.160371</td>\n",
       "      <td>0.136043</td>\n",
       "      <td>0.225657</td>\n",
       "      <td>86.47</td>\n",
       "      <td>298.330000</td>\n",
       "      <td>...</td>\n",
       "      <td>207.10</td>\n",
       "      <td>91.600000</td>\n",
       "      <td>86.47</td>\n",
       "      <td>18.070000</td>\n",
       "      <td>7.471429</td>\n",
       "      <td>27.433333</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>34.7</td>\n",
       "      <td>21.7</td>\n",
       "      <td>36.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>23</td>\n",
       "      <td>2010-06-11</td>\n",
       "      <td>0.247057</td>\n",
       "      <td>0.146057</td>\n",
       "      <td>0.250357</td>\n",
       "      <td>0.233714</td>\n",
       "      <td>58.94</td>\n",
       "      <td>296.598571</td>\n",
       "      <td>...</td>\n",
       "      <td>50.60</td>\n",
       "      <td>94.280000</td>\n",
       "      <td>58.94</td>\n",
       "      <td>17.008571</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>32.2</td>\n",
       "      <td>19.2</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>24</td>\n",
       "      <td>2010-06-18</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.245771</td>\n",
       "      <td>0.278886</td>\n",
       "      <td>0.325486</td>\n",
       "      <td>59.67</td>\n",
       "      <td>296.345714</td>\n",
       "      <td>...</td>\n",
       "      <td>62.33</td>\n",
       "      <td>94.660000</td>\n",
       "      <td>59.67</td>\n",
       "      <td>16.815714</td>\n",
       "      <td>7.871429</td>\n",
       "      <td>25.433333</td>\n",
       "      <td>8.733333</td>\n",
       "      <td>31.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>25</td>\n",
       "      <td>2010-06-25</td>\n",
       "      <td>0.298186</td>\n",
       "      <td>0.232971</td>\n",
       "      <td>0.274214</td>\n",
       "      <td>0.315757</td>\n",
       "      <td>63.22</td>\n",
       "      <td>298.097143</td>\n",
       "      <td>...</td>\n",
       "      <td>36.90</td>\n",
       "      <td>89.082857</td>\n",
       "      <td>63.22</td>\n",
       "      <td>17.355714</td>\n",
       "      <td>11.014286</td>\n",
       "      <td>27.475000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>33.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1456 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "0      sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n",
       "1      sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n",
       "2      sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n",
       "3      sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n",
       "4      sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n",
       "...   ...   ...         ...             ...       ...       ...       ...   \n",
       "1451   iq  2010          21      2010-05-28  0.342750  0.318900  0.256343   \n",
       "1452   iq  2010          22      2010-06-04  0.160157  0.160371  0.136043   \n",
       "1453   iq  2010          23      2010-06-11  0.247057  0.146057  0.250357   \n",
       "1454   iq  2010          24      2010-06-18  0.333914  0.245771  0.278886   \n",
       "1455   iq  2010          25      2010-06-25  0.298186  0.232971  0.274214   \n",
       "\n",
       "       ndvi_sw  prec_amt_mm  re_an_air_temp_k  ...  \\\n",
       "0     0.177617        12.42        297.572857  ...   \n",
       "1     0.155486        22.82        298.211429  ...   \n",
       "2     0.170843        34.54        298.781429  ...   \n",
       "3     0.235886        15.36        298.987143  ...   \n",
       "4     0.247340         7.52        299.518571  ...   \n",
       "...        ...          ...               ...  ...   \n",
       "1451  0.292514        55.30        299.334286  ...   \n",
       "1452  0.225657        86.47        298.330000  ...   \n",
       "1453  0.233714        58.94        296.598571  ...   \n",
       "1454  0.325486        59.67        296.345714  ...   \n",
       "1455  0.315757        63.22        298.097143  ...   \n",
       "\n",
       "      re_an_precip_amt_kg_per_m2  re_an_relative_hd_percent  \\\n",
       "0                          32.00                  73.365714   \n",
       "1                          17.94                  77.368571   \n",
       "2                          26.10                  82.052857   \n",
       "3                          13.90                  80.337143   \n",
       "4                          12.20                  80.460000   \n",
       "...                          ...                        ...   \n",
       "1451                       45.00                  88.765714   \n",
       "1452                      207.10                  91.600000   \n",
       "1453                       50.60                  94.280000   \n",
       "1454                       62.33                  94.660000   \n",
       "1455                       36.90                  89.082857   \n",
       "\n",
       "      re_an_sat_precip_amt_mm  re_an_specific_hd_g_per_kg  re_an_tdtr_k  \\\n",
       "0                       12.42                   14.012857      2.628571   \n",
       "1                       22.82                   15.372857      2.371429   \n",
       "2                       34.54                   16.848571      2.300000   \n",
       "3                       15.36                   16.672857      2.428571   \n",
       "4                        7.52                   17.210000      3.014286   \n",
       "...                       ...                         ...           ...   \n",
       "1451                    55.30                   18.485714      9.800000   \n",
       "1452                    86.47                   18.070000      7.471429   \n",
       "1453                    58.94                   17.008571      7.500000   \n",
       "1454                    59.67                   16.815714      7.871429   \n",
       "1455                    63.22                   17.355714     11.014286   \n",
       "\n",
       "      stn_avg_temp_c  stn_diur_temp_rng_c  stn_max_temp_c  stn_min_temp_c  \\\n",
       "0          25.442857             6.900000            29.4            20.0   \n",
       "1          26.714286             6.371429            31.7            22.2   \n",
       "2          26.714286             6.485714            32.2            22.8   \n",
       "3          27.471429             6.771429            33.3            23.3   \n",
       "4          28.942857             9.371429            35.0            23.9   \n",
       "...              ...                  ...             ...             ...   \n",
       "1451       28.633333            11.933333            35.4            22.4   \n",
       "1452       27.433333            10.500000            34.7            21.7   \n",
       "1453       24.400000             6.900000            32.2            19.2   \n",
       "1454       25.433333             8.733333            31.2            21.0   \n",
       "1455       27.475000             9.900000            33.7            22.2   \n",
       "\n",
       "      stn_precip_mm  \n",
       "0              16.0  \n",
       "1               8.6  \n",
       "2              41.4  \n",
       "3               4.0  \n",
       "4               5.8  \n",
       "...             ...  \n",
       "1451           27.0  \n",
       "1452           36.6  \n",
       "1453            7.4  \n",
       "1454           16.0  \n",
       "1455           20.4  \n",
       "\n",
       "[1456 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'station': 'stn', 'reanalysis': 're_an','humidity': 'hd','precipitation':'prec'}\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text.columns = text.columns.str.replace(i, j)\n",
    "    return text\n",
    "\n",
    "replace_all(df,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.year = df.year.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = pd.read_csv('dengue_labels_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  city  year  weekofyear  total_cases\n",
       "0   sj  1990          18            4\n",
       "1   sj  1990          19            5\n",
       "2   sj  1990          20            4\n",
       "3   sj  1990          21            3\n",
       "4   sj  1990          22            6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.year = lb.year.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df.merge(lb,on=['city','year','weekofyear'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>prec_amt_mm</th>\n",
       "      <th>re_an_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>re_an_relative_hd_percent</th>\n",
       "      <th>re_an_sat_precip_amt_mm</th>\n",
       "      <th>re_an_specific_hd_g_per_kg</th>\n",
       "      <th>re_an_tdtr_k</th>\n",
       "      <th>stn_avg_temp_c</th>\n",
       "      <th>stn_diur_temp_rng_c</th>\n",
       "      <th>stn_max_temp_c</th>\n",
       "      <th>stn_min_temp_c</th>\n",
       "      <th>stn_precip_mm</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>1990-04-30</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.198483</td>\n",
       "      <td>0.177617</td>\n",
       "      <td>12.42</td>\n",
       "      <td>297.572857</td>\n",
       "      <td>...</td>\n",
       "      <td>73.365714</td>\n",
       "      <td>12.42</td>\n",
       "      <td>14.012857</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>25.442857</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>29.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>1990-05-07</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.142175</td>\n",
       "      <td>0.162357</td>\n",
       "      <td>0.155486</td>\n",
       "      <td>22.82</td>\n",
       "      <td>298.211429</td>\n",
       "      <td>...</td>\n",
       "      <td>77.368571</td>\n",
       "      <td>22.82</td>\n",
       "      <td>15.372857</td>\n",
       "      <td>2.371429</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.371429</td>\n",
       "      <td>31.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>1990-05-14</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>34.54</td>\n",
       "      <td>298.781429</td>\n",
       "      <td>...</td>\n",
       "      <td>82.052857</td>\n",
       "      <td>34.54</td>\n",
       "      <td>16.848571</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.485714</td>\n",
       "      <td>32.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>41.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>1990-05-21</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>0.245067</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.235886</td>\n",
       "      <td>15.36</td>\n",
       "      <td>298.987143</td>\n",
       "      <td>...</td>\n",
       "      <td>80.337143</td>\n",
       "      <td>15.36</td>\n",
       "      <td>16.672857</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>27.471429</td>\n",
       "      <td>6.771429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>1990-05-28</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.247340</td>\n",
       "      <td>7.52</td>\n",
       "      <td>299.518571</td>\n",
       "      <td>...</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>7.52</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>3.014286</td>\n",
       "      <td>28.942857</td>\n",
       "      <td>9.371429</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>21</td>\n",
       "      <td>2010-05-28</td>\n",
       "      <td>0.342750</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.256343</td>\n",
       "      <td>0.292514</td>\n",
       "      <td>55.30</td>\n",
       "      <td>299.334286</td>\n",
       "      <td>...</td>\n",
       "      <td>88.765714</td>\n",
       "      <td>55.30</td>\n",
       "      <td>18.485714</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>28.633333</td>\n",
       "      <td>11.933333</td>\n",
       "      <td>35.4</td>\n",
       "      <td>22.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>22</td>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>0.160157</td>\n",
       "      <td>0.160371</td>\n",
       "      <td>0.136043</td>\n",
       "      <td>0.225657</td>\n",
       "      <td>86.47</td>\n",
       "      <td>298.330000</td>\n",
       "      <td>...</td>\n",
       "      <td>91.600000</td>\n",
       "      <td>86.47</td>\n",
       "      <td>18.070000</td>\n",
       "      <td>7.471429</td>\n",
       "      <td>27.433333</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>34.7</td>\n",
       "      <td>21.7</td>\n",
       "      <td>36.6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>23</td>\n",
       "      <td>2010-06-11</td>\n",
       "      <td>0.247057</td>\n",
       "      <td>0.146057</td>\n",
       "      <td>0.250357</td>\n",
       "      <td>0.233714</td>\n",
       "      <td>58.94</td>\n",
       "      <td>296.598571</td>\n",
       "      <td>...</td>\n",
       "      <td>94.280000</td>\n",
       "      <td>58.94</td>\n",
       "      <td>17.008571</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>32.2</td>\n",
       "      <td>19.2</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>24</td>\n",
       "      <td>2010-06-18</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.245771</td>\n",
       "      <td>0.278886</td>\n",
       "      <td>0.325486</td>\n",
       "      <td>59.67</td>\n",
       "      <td>296.345714</td>\n",
       "      <td>...</td>\n",
       "      <td>94.660000</td>\n",
       "      <td>59.67</td>\n",
       "      <td>16.815714</td>\n",
       "      <td>7.871429</td>\n",
       "      <td>25.433333</td>\n",
       "      <td>8.733333</td>\n",
       "      <td>31.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>25</td>\n",
       "      <td>2010-06-25</td>\n",
       "      <td>0.298186</td>\n",
       "      <td>0.232971</td>\n",
       "      <td>0.274214</td>\n",
       "      <td>0.315757</td>\n",
       "      <td>63.22</td>\n",
       "      <td>298.097143</td>\n",
       "      <td>...</td>\n",
       "      <td>89.082857</td>\n",
       "      <td>63.22</td>\n",
       "      <td>17.355714</td>\n",
       "      <td>11.014286</td>\n",
       "      <td>27.475000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>33.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>20.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1456 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "0      sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n",
       "1      sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n",
       "2      sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n",
       "3      sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n",
       "4      sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n",
       "...   ...   ...         ...             ...       ...       ...       ...   \n",
       "1451   iq  2010          21      2010-05-28  0.342750  0.318900  0.256343   \n",
       "1452   iq  2010          22      2010-06-04  0.160157  0.160371  0.136043   \n",
       "1453   iq  2010          23      2010-06-11  0.247057  0.146057  0.250357   \n",
       "1454   iq  2010          24      2010-06-18  0.333914  0.245771  0.278886   \n",
       "1455   iq  2010          25      2010-06-25  0.298186  0.232971  0.274214   \n",
       "\n",
       "       ndvi_sw  prec_amt_mm  re_an_air_temp_k  ...  re_an_relative_hd_percent  \\\n",
       "0     0.177617        12.42        297.572857  ...                  73.365714   \n",
       "1     0.155486        22.82        298.211429  ...                  77.368571   \n",
       "2     0.170843        34.54        298.781429  ...                  82.052857   \n",
       "3     0.235886        15.36        298.987143  ...                  80.337143   \n",
       "4     0.247340         7.52        299.518571  ...                  80.460000   \n",
       "...        ...          ...               ...  ...                        ...   \n",
       "1451  0.292514        55.30        299.334286  ...                  88.765714   \n",
       "1452  0.225657        86.47        298.330000  ...                  91.600000   \n",
       "1453  0.233714        58.94        296.598571  ...                  94.280000   \n",
       "1454  0.325486        59.67        296.345714  ...                  94.660000   \n",
       "1455  0.315757        63.22        298.097143  ...                  89.082857   \n",
       "\n",
       "      re_an_sat_precip_amt_mm  re_an_specific_hd_g_per_kg  re_an_tdtr_k  \\\n",
       "0                       12.42                   14.012857      2.628571   \n",
       "1                       22.82                   15.372857      2.371429   \n",
       "2                       34.54                   16.848571      2.300000   \n",
       "3                       15.36                   16.672857      2.428571   \n",
       "4                        7.52                   17.210000      3.014286   \n",
       "...                       ...                         ...           ...   \n",
       "1451                    55.30                   18.485714      9.800000   \n",
       "1452                    86.47                   18.070000      7.471429   \n",
       "1453                    58.94                   17.008571      7.500000   \n",
       "1454                    59.67                   16.815714      7.871429   \n",
       "1455                    63.22                   17.355714     11.014286   \n",
       "\n",
       "      stn_avg_temp_c  stn_diur_temp_rng_c  stn_max_temp_c  stn_min_temp_c  \\\n",
       "0          25.442857             6.900000            29.4            20.0   \n",
       "1          26.714286             6.371429            31.7            22.2   \n",
       "2          26.714286             6.485714            32.2            22.8   \n",
       "3          27.471429             6.771429            33.3            23.3   \n",
       "4          28.942857             9.371429            35.0            23.9   \n",
       "...              ...                  ...             ...             ...   \n",
       "1451       28.633333            11.933333            35.4            22.4   \n",
       "1452       27.433333            10.500000            34.7            21.7   \n",
       "1453       24.400000             6.900000            32.2            19.2   \n",
       "1454       25.433333             8.733333            31.2            21.0   \n",
       "1455       27.475000             9.900000            33.7            22.2   \n",
       "\n",
       "      stn_precip_mm  total_cases  \n",
       "0              16.0            4  \n",
       "1               8.6            5  \n",
       "2              41.4            4  \n",
       "3               4.0            3  \n",
       "4               5.8            6  \n",
       "...             ...          ...  \n",
       "1451           27.0            5  \n",
       "1452           36.6            8  \n",
       "1453            7.4            1  \n",
       "1454           16.0            1  \n",
       "1455           20.4            4  \n",
       "\n",
       "[1456 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filledna = df_merged.fillna(method='ffill',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"total_cases\"] = df_merged[\"total_cases\"].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_merged.drop('total_cases',axis=1)\n",
    "y = df_merged['total_cases']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make an 80-20 stratified split based on the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4.0\n",
       "1       5.0\n",
       "2       4.0\n",
       "3       3.0\n",
       "4       6.0\n",
       "       ... \n",
       "1451    5.0\n",
       "1452    8.0\n",
       "1453    1.0\n",
       "1454    1.0\n",
       "1455    4.0\n",
       "Name: total_cases, Length: 1456, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0      100\n",
       "6.0       71\n",
       "5.0       70\n",
       "3.0       70\n",
       "2.0       69\n",
       "        ... \n",
       "131.0      1\n",
       "149.0      1\n",
       "329.0      1\n",
       "169.0      1\n",
       "333.0      1\n",
       "Name: total_cases, Length: 135, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use stratify sampling, will need to remove the y values where the frequency is 1.  \n",
    "Otherwise we get following error:  \n",
    "\n",
    "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
    "\n",
    "https://github.com/davidsbatista/text-classification/issues/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the reords which do not have frequency greater than 1\n",
    "df_merged.drop( ['week_start_date'],1,inplace=True)\n",
    "df_merged =  df_merged.groupby('total_cases').filter(lambda x: len(x) > 1)\n",
    "\n",
    "X = df_merged.drop('total_cases',axis=1)\n",
    "y = df_merged['total_cases']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0      100\n",
       "6.0       71\n",
       "5.0       70\n",
       "3.0       70\n",
       "2.0       69\n",
       "        ... \n",
       "82.0       2\n",
       "106.0      2\n",
       "129.0      2\n",
       "85.0       2\n",
       "116.0      2\n",
       "Name: total_cases, Length: 88, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess the data (Encode the categorical features and Standardize the numerical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "cat_feature = [\"city\", 'year']\n",
    "#convert the attributes to categorical form\n",
    "for i in cat_feature:\n",
    "    X_train[i] = X_train[i].astype(\"category\")\n",
    "    X_test[i] = X_test[i].astype(\"category\")\n",
    "\n",
    "#Convert categorical variable into dummy/indicator variables\n",
    "X_train_temp = pd.get_dummies(X_train[['city','year']])\n",
    "X_test_temp = pd.get_dummies(X_test[['city','year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#Applying the StandardScaler to all the numerical data\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(columns=['city', 'year']))\n",
    "new_X_train = np.concatenate((X_train_scaled,np.array(X_train_temp)),axis=1)\n",
    "               \n",
    "X_test_scaled = scaler.transform(X_test.drop(columns=['city', 'year']))\n",
    "new_X_test = np.concatenate((X_test_scaled,np.array(X_test_temp)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a stochastic gradient descent regressor, train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=42,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_reg.fit(new_X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your model based on applicable metrics. Show the metric(s) you chose and why you chose this(these) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = sgd_reg.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.340553228835818"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_predict)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we don't want out model to be error is not that sensitive to outliers, using MAE makes sense so that it doesn't penalize huge errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List the hyper-parameters that can be tuned in SGD. Show the code along with comments on the parameter value chosen \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=True, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=42,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg_1 = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42,early_stopping=True )\n",
    "sgd_reg_1.fit(new_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_1 = sgd_reg_1.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.660748412916734"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_predict_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model, I have used the basic hyperparamets.   \n",
    "max_iter = 1000  \n",
    "Learning rate = 0.0001  \n",
    "Early_stopping = True   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=True, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=5000,\n",
       "             n_iter_no_change=5, penalty='l1', power_t=0.25, random_state=42,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg_2 = SGDRegressor(max_iter=5000, tol=1e-3, random_state=42,early_stopping=True,penalty='l1' )\n",
    "sgd_reg_2.fit(new_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_2 = sgd_reg_2.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.660667095499635"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_predict_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, model, I added loss='huber' along with penalty='l1' But using Huber loss fuction, increases the error. \n",
    "As we can see, using  penalty='l2'or  penalty='l1' doesn't make any difference on the absolute Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.341998854816056"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg_4 = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42 , penalty=None)\n",
    "sgd_reg_4.fit(new_X_train,y_train)\n",
    "y_predict_4 = sgd_reg_4.predict(new_X_test)\n",
    "mean_absolute_error(y_test, y_predict_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't penalize the model, the error is decreasesing but it's safe to penalize the model so that it doesn't overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Learning curve and provide insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MAE')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_errors, test_errors = [], []\n",
    "for m in range(1, len(new_X_train)):\n",
    "    sgd_reg.fit(X_train_scaled[:m], y_train[:m])\n",
    "    y_train_predict = sgd_reg.predict(X_train_scaled[:m])\n",
    "    y_test_predict = sgd_reg.predict(X_test_scaled)\n",
    "    train_errors.append(mean_absolute_error(y_train[:m], y_train_predict))\n",
    "    test_errors.append(mean_absolute_error(y_test, y_test_predict))\n",
    "plt.plot(train_errors, \"r+-\", linewidth=2, label = \"Train\")\n",
    "plt.plot(test_errors, \"b-\", linewidth=3, label = \"Test\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14) \n",
    "plt.xlabel(\"Training set size\", fontsize=14)\n",
    "plt.ylabel(\"MAE\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after test size of 200 it has flattern. i.e. training set doesn’t make the average error much better or worse. Looking at the Test MAE, after a point it is lower than the train MAE  \n",
    "This can happen in the following cases:  \n",
    "1. It is possible when you have not sampled the data or split the test train data perfectly.  \n",
    "2. It is possible when your test data is small and its not a good representative of train data, then there may or may    not be a case when for that test data it behaves good and gives low error.  \n",
    "3. There can be a case when you use regularization and your train data move away from overfitting i.e. increased        train error but generalizing good on test data i.e. low train error.  \n",
    "\n",
    "Source: StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>16.435167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>sj</td>\n",
       "      <td>1993</td>\n",
       "      <td>29</td>\n",
       "      <td>28.590932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>27</td>\n",
       "      <td>29.216370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>iq</td>\n",
       "      <td>2009</td>\n",
       "      <td>27</td>\n",
       "      <td>5.342592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>iq</td>\n",
       "      <td>2009</td>\n",
       "      <td>45</td>\n",
       "      <td>13.670097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>sj</td>\n",
       "      <td>1996</td>\n",
       "      <td>2</td>\n",
       "      <td>14.275543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>sj</td>\n",
       "      <td>2007</td>\n",
       "      <td>52</td>\n",
       "      <td>33.724722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>iq</td>\n",
       "      <td>2002</td>\n",
       "      <td>39</td>\n",
       "      <td>4.765269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>iq</td>\n",
       "      <td>2004</td>\n",
       "      <td>34</td>\n",
       "      <td>1.489343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>sj</td>\n",
       "      <td>1998</td>\n",
       "      <td>28</td>\n",
       "      <td>42.437435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     city  year  weekofyear       pred\n",
       "921    sj  2008           3  16.435167\n",
       "168    sj  1993          29  28.590932\n",
       "9      sj  1990          27  29.216370\n",
       "1404   iq  2009          27   5.342592\n",
       "1422   iq  2009          45  13.670097\n",
       "...   ...   ...         ...        ...\n",
       "296    sj  1996           2  14.275543\n",
       "918    sj  2007          52  33.724722\n",
       "1052   iq  2002          39   4.765269\n",
       "1151   iq  2004          34   1.489343\n",
       "426    sj  1998          28  42.437435\n",
       "\n",
       "[282 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output =  {\"city\" : X_test[\"city\"],\n",
    "           \"year\" : X_test[\"year\"],\n",
    "           \"weekofyear\" : X_test[\"weekofyear\"],\n",
    "           \"pred\" : y_predict_2}\n",
    "\n",
    "df = pd.DataFrame(output, columns= ['city', 'year','weekofyear','pred'])\n",
    "\n",
    "df.to_csv (r'export_dataframe.csv', index = False, header=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
